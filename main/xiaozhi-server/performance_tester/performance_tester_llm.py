import asyncio
import logging
import os
import sys
import statistics
import time
import concurrent.futures
from typing import Dict, Optional
import yaml
import aiohttp
from tabulate import tabulate

# Th√™m th∆∞ m·ª•c g·ªëc d·ª± √°n v√†o ƒë∆∞·ªùng d·∫´n Python
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, ".."))
sys.path.insert(0, project_root)

from core.utils.llm import create_instance as create_llm_instance
from config.settings import load_config

# Thi·∫øt l·∫≠p m·ª©c log to√†n c·ª•c l√† WARNING, ·ª©c ch·∫ø log m·ª©c INFO
logging.basicConfig(level=logging.WARNING)

description = "Ki·ªÉm tra hi·ªáu su·∫•t m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn"


class LLMPerformanceTester:
    def __init__(self):
        self.config = load_config()
        # S·ª≠ d·ª•ng n·ªôi dung ki·ªÉm tra ph√π h·ª£p h∆°n v·ªõi k·ªãch b·∫£n agent, bao g·ªìm system prompt
        self.system_prompt = self._load_system_prompt()
        self.test_sentences = self.config.get("module_test", {}).get(
            "test_sentences",
            [
                "Xin ch√†o, h√¥m nay t√¥i c·∫£m th·∫•y kh√¥ng t·ªët l·∫Øm, b·∫°n c√≥ th·ªÉ an ·ªßi t√¥i kh√¥ng?",
                "Gi√∫p t√¥i ki·ªÉm tra th·ªùi ti·∫øt ng√†y mai nh∆∞ th·∫ø n√†o?",
                "T√¥i mu·ªën nghe m·ªôt c√¢u chuy·ªán th√∫ v·ªã, b·∫°n c√≥ th·ªÉ k·ªÉ cho t√¥i nghe kh√¥ng?",
                "B√¢y gi·ªù l√† m·∫•y gi·ªù? H√¥m nay l√† th·ª© m·∫•y?",
                "T√¥i mu·ªën ƒë·∫∑t m·ªôt b√°o th·ª©c 8 gi·ªù s√°ng ng√†y mai ƒë·ªÉ nh·∫Øc t√¥i h·ªçp",
            ],
        )
        self.results = {}

    def _load_system_prompt(self) -> str:
        """T·∫£i system prompt"""
        try:
            prompt_file = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), self.config.get("prompt_template", "agent-base-prompt.txt")
            )
            with open(prompt_file, "r", encoding="utf-8") as f:
                content = f.read()
                # Thay th·∫ø bi·∫øn template b·∫±ng gi√° tr·ªã test
                content = content.replace(
                    "{{base_prompt}}", "B·∫°n l√† XiaoZhi, m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† d·ªÖ th∆∞∆°ng"
                )
                content = content.replace(
                    "{{emojiList}}", "üòÄ,üòÉ,üòÑ,üòÅ,üòä,üòç,ü§î,üòÆ,üò±,üò¢,üò≠,üò¥,üòµ,ü§ó,üôÑ"
                )
                content = content.replace("{{current_time}}", "17 th√°ng 8 nƒÉm 2024 12:30:45")
                content = content.replace("{{today_date}}", "17 th√°ng 8 nƒÉm 2024")
                content = content.replace("{{today_weekday}}", "Th·ª© B·∫£y")
                content = content.replace("{{lunar_date}}", "Gi√°p Th√¨n nƒÉm th√°ng 7 ng√†y 14")
                content = content.replace("{{local_address}}", "H√† N·ªôi")
                content = content.replace("{{weather_info}}", "H√¥m nay n·∫Øng, 25-32‚ÑÉ")
                return content
        except Exception as e:
            print(f"Kh√¥ng th·ªÉ t·∫£i file system prompt: {e}")
            return "B·∫°n l√† XiaoZhi, m·ªôt tr·ª£ l√Ω AI th√¥ng minh v√† d·ªÖ th∆∞∆°ng. Vui l√≤ng tr·∫£ l·ªùi ng∆∞·ªùi d√πng b·∫±ng gi·ªçng ƒëi·ªáu ·∫•m √°p v√† th√¢n thi·ªán."

    def _collect_response_sync(self, llm, messages, llm_name, sentence_start):
        """Ph∆∞∆°ng th·ª©c h·ªó tr·ª£ thu th·∫≠p d·ªØ li·ªáu ph·∫£n h·ªìi ƒë·ªìng b·ªô"""
        chunks = []
        first_token_received = False
        first_token_time = None

        try:
            response_generator = llm.response("perf_test", messages)
            chunk_count = 0
            for chunk in response_generator:
                chunk_count += 1
                # Ki·ªÉm tra xem c√≥ n√™n ng·∫Øt sau khi x·ª≠ l√Ω m·ªôt s·ªë l∆∞·ª£ng chunk nh·∫•t ƒë·ªãnh
                if chunk_count % 10 == 0:
                    # Tho√°t s·ªõm b·∫±ng c√°ch ki·ªÉm tra xem thread hi·ªán t·∫°i c√≥ ƒë∆∞·ª£c ƒë√°nh d·∫•u l√† ng·∫Øt kh√¥ng
                    import threading

                    if (
                        threading.current_thread().ident
                        != threading.main_thread().ident
                    ):
                        # N·∫øu kh√¥ng ph·∫£i thread ch√≠nh, ki·ªÉm tra xem c√≥ n√™n d·ª´ng kh√¥ng
                        pass

                # Ki·ªÉm tra xem chunk c√≥ ch·ª©a th√¥ng tin l·ªói kh√¥ng
                chunk_str = str(chunk)
                if (
                    "ÂºÇÂ∏∏" in chunk_str
                    or "ÈîôËØØ" in chunk_str
                    or "502" in chunk_str.lower()
                ):
                    error_msg = chunk_str.lower()
                    print(f"{llm_name} ph·∫£n h·ªìi ch·ª©a th√¥ng tin l·ªói: {error_msg}")
                    # N√©m exception ch·ª©a th√¥ng tin l·ªói
                    raise Exception(chunk_str)

                if not first_token_received and chunk.strip() != "":
                    first_token_time = time.time() - sentence_start
                    first_token_received = True
                    print(f"{llm_name} Token ƒë·∫ßu ti√™n: {first_token_time:.3f}s")
                chunks.append(chunk)
        except Exception as e:
            # Th√¥ng tin l·ªói chi ti·∫øt h∆°n
            error_msg = str(e).lower()
            print(f"{llm_name} thu th·∫≠p ph·∫£n h·ªìi b·∫•t th∆∞·ªùng: {error_msg}")
            # ƒê·ªëi v·ªõi l·ªói 502 ho·∫∑c l·ªói m·∫°ng, n√©m exception tr·ª±c ti·∫øp ƒë·ªÉ l·ªõp tr√™n x·ª≠ l√Ω
            if (
                "502" in error_msg
                or "bad gateway" in error_msg
                or "error code: 502" in error_msg
                or "ÂºÇÂ∏∏" in str(e)
                or "ÈîôËØØ" in str(e)
            ):
                raise e
            # ƒê·ªëi v·ªõi c√°c l·ªói kh√°c, c√≥ th·ªÉ tr·∫£ v·ªÅ k·∫øt qu·∫£ m·ªôt ph·∫ßn
            return chunks, first_token_time

        return chunks, first_token_time

    async def _check_ollama_service(self, base_url: str, model_name: str) -> bool:
        """Ki·ªÉm tra tr·∫°ng th√°i d·ªãch v·ª• Ollama b·∫•t ƒë·ªìng b·ªô"""
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(f"{base_url}/api/version") as response:
                    if response.status != 200:
                        print(f"D·ªãch v·ª• Ollama ch∆∞a kh·ªüi ƒë·ªông ho·∫∑c kh√¥ng th·ªÉ truy c·∫≠p: {base_url}")
                        return False
                async with session.get(f"{base_url}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = data.get("models", [])
                        if not any(model["name"] == model_name for model in models):
                            print(
                                f"M√¥ h√¨nh Ollama {model_name} kh√¥ng t√¨m th·∫•y, vui l√≤ng t·∫£i xu·ªëng tr∆∞·ªõc b·∫±ng `ollama pull {model_name}`"
                            )
                            return False
                    else:
                        print("Kh√¥ng th·ªÉ l·∫•y danh s√°ch m√¥ h√¨nh Ollama")
                        return False
                return True
            except Exception as e:
                print(f"Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn d·ªãch v·ª• Ollama: {str(e)}")
                return False

    async def _test_single_sentence(
        self, llm_name: str, llm, sentence: str
    ) -> Optional[Dict]:
        """Ki·ªÉm tra hi·ªáu su·∫•t m·ªôt c√¢u ƒë∆°n l·∫ª"""
        try:
            print(f"{llm_name} b·∫Øt ƒë·∫ßu ki·ªÉm tra: {sentence[:20]}...")
            sentence_start = time.time()
            first_token_received = False
            first_token_time = None

            # T·∫°o message bao g·ªìm system prompt
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": sentence},
            ]

            # S·ª≠ d·ª•ng asyncio.wait_for ƒë·ªÉ ki·ªÉm so√°t timeout
            try:
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    # T·∫°o task thu th·∫≠p ph·∫£n h·ªìi
                    future = executor.submit(
                        self._collect_response_sync,
                        llm,
                        messages,
                        llm_name,
                        sentence_start,
                    )

                    # S·ª≠ d·ª•ng asyncio.wait_for ƒë·ªÉ th·ª±c hi·ªán ki·ªÉm so√°t timeout
                    try:
                        response_chunks, first_token_time = await asyncio.wait_for(
                            asyncio.wrap_future(future), timeout=10.0
                        )
                    except asyncio.TimeoutError:
                        print(f"{llm_name} ki·ªÉm tra timeout (10 gi√¢y), b·ªè qua")
                        # Bu·ªôc h·ªßy future
                        future.cancel()
                        # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ ƒë·∫£m b·∫£o task trong thread pool c√≥ th·ªÉ ph·∫£n h·ªìi h·ªßy
                        try:
                            await asyncio.wait_for(
                                asyncio.wrap_future(future), timeout=1.0
                            )
                        except (
                            asyncio.TimeoutError,
                            concurrent.futures.CancelledError,
                            Exception,
                        ):
                            # B·ªè qua t·∫•t c·∫£ exception, ƒë·∫£m b·∫£o ch∆∞∆°ng tr√¨nh ti·∫øp t·ª•c th·ª±c thi
                            pass
                        return None

            except Exception as timeout_error:
                print(f"{llm_name} x·ª≠ l√Ω b·∫•t th∆∞·ªùng: {timeout_error}")
                return None

            response_time = time.time() - sentence_start
            print(f"{llm_name} ho√†n th√†nh ph·∫£n h·ªìi: {response_time:.3f}s")

            return {
                "name": llm_name,
                "type": "llm",
                "first_token_time": first_token_time,
                "response_time": response_time,
            }
        except Exception as e:
            error_msg = str(e).lower()
            # Ki·ªÉm tra xem c√≥ ph·∫£i l·ªói 502 ho·∫∑c l·ªói m·∫°ng kh√¥ng
            if (
                "502" in error_msg
                or "bad gateway" in error_msg
                or "error code: 502" in error_msg
            ):
                print(f"{llm_name} g·∫∑p l·ªói 502, b·ªè qua ki·ªÉm tra")
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "L·ªói m·∫°ng 502",
                }
            print(f"{llm_name} ki·ªÉm tra c√¢u th·∫•t b·∫°i: {str(e)}")
            return None

    async def _test_llm(self, llm_name: str, config: Dict) -> Dict:
        """Ki·ªÉm tra hi·ªáu su·∫•t m·ªôt LLM ƒë∆°n l·∫ª b·∫•t ƒë·ªìng b·ªô"""
        try:
            # ƒê·ªëi v·ªõi Ollama, b·ªè qua ki·ªÉm tra api_key v√† x·ª≠ l√Ω ƒë·∫∑c bi·ªát
            if llm_name == "Ollama":
                base_url = config.get("base_url", "http://localhost:11434")
                model_name = config.get("model_name")
                if not model_name:
                    print("Ollama ch∆∞a c·∫•u h√¨nh model_name")
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "L·ªói m·∫°ng",
                    }

                if not await self._check_ollama_service(base_url, model_name):
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "L·ªói m·∫°ng",
                    }
            else:
                if "api_key" in config and any(
                    x in config["api_key"] for x in ["‰Ω†ÁöÑ", "placeholder", "sk-xxx"]
                ):
                    print(f"B·ªè qua LLM ch∆∞a c·∫•u h√¨nh: {llm_name}")
                    return {
                        "name": llm_name,
                        "type": "llm",
                        "errors": 1,
                        "error_type": "L·ªói c·∫•u h√¨nh",
                    }

            # L·∫•y lo·∫°i th·ª±c t·∫ø (t∆∞∆°ng th√≠ch v·ªõi c·∫•u h√¨nh c≈©)
            module_type = config.get("type", llm_name)
            llm = create_llm_instance(module_type, config)

            # S·ª≠ d·ª•ng m√£ h√≥a UTF-8 th·ªëng nh·∫•t
            test_sentences = [
                s.encode("utf-8").decode("utf-8") for s in self.test_sentences
            ]

            # T·∫°o task ki·ªÉm tra cho t·∫•t c·∫£ c√°c c√¢u
            sentence_tasks = []
            for sentence in test_sentences:
                sentence_tasks.append(
                    self._test_single_sentence(llm_name, llm, sentence)
                )

            # Th·ª±c thi t·∫•t c·∫£ c√°c ki·ªÉm tra c√¢u ƒë·ªìng th·ªùi, v√† x·ª≠ l√Ω c√°c exception c√≥ th·ªÉ x·∫£y ra
            sentence_results = await asyncio.gather(
                *sentence_tasks, return_exceptions=True
            )

            # X·ª≠ l√Ω k·∫øt qu·∫£, l·ªçc b·ªè exception v√† gi√° tr·ªã None
            valid_results = []
            for result in sentence_results:
                if isinstance(result, dict) and result is not None:
                    valid_results.append(result)
                elif isinstance(result, Exception):
                    error_msg = str(result).lower()
                    if "502" in error_msg or "bad gateway" in error_msg:
                        print(f"{llm_name} g·∫∑p l·ªói 502, b·ªè qua ki·ªÉm tra c√¢u ƒë√≥")
                        return {
                            "name": llm_name,
                            "type": "llm",
                            "errors": 1,
                            "error_type": "L·ªói m·∫°ng 502",
                        }
                    else:
                        print(f"{llm_name} ki·ªÉm tra c√¢u b·∫•t th∆∞·ªùng: {result}")

            if not valid_results:
                print(f"{llm_name} kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá, c√≥ th·ªÉ g·∫∑p v·∫•n ƒë·ªÅ m·∫°ng ho·∫∑c l·ªói c·∫•u h√¨nh")
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "L·ªói m·∫°ng",
                }

            # Ki·ªÉm tra s·ªë l∆∞·ª£ng k·∫øt qu·∫£ h·ª£p l·ªá, n·∫øu qu√° √≠t th√¨ coi nh∆∞ test th·∫•t b·∫°i
            if len(valid_results) < len(test_sentences) * 0.3:  # √çt nh·∫•t ph·∫£i c√≥ 30% t·ª∑ l·ªá th√†nh c√¥ng
                print(
                    f"{llm_name} s·ªë c√¢u test th√†nh c√¥ng qu√° √≠t ({len(valid_results)}/{len(test_sentences)}), c√≥ th·ªÉ m·∫°ng kh√¥ng ·ªïn ƒë·ªãnh ho·∫∑c API c√≥ v·∫•n ƒë·ªÅ"
                )
                return {
                    "name": llm_name,
                    "type": "llm",
                    "errors": 1,
                    "error_type": "L·ªói m·∫°ng",
                }

            first_token_times = [
                r["first_token_time"]
                for r in valid_results
                if r.get("first_token_time")
            ]
            response_times = [r["response_time"] for r in valid_results]

            # L·ªçc d·ªØ li·ªáu b·∫•t th∆∞·ªùng (d·ªØ li·ªáu v∆∞·ª£t qu√° 3 ƒë·ªô l·ªách chu·∫©n)
            if len(response_times) > 1:
                mean = statistics.mean(response_times)
                stdev = statistics.stdev(response_times)
                filtered_times = [t for t in response_times if t <= mean + 3 * stdev]
            else:
                filtered_times = response_times

            return {
                "name": llm_name,
                "type": "llm",
                "avg_response": sum(response_times) / len(response_times),
                "avg_first_token": (
                    sum(first_token_times) / len(first_token_times)
                    if first_token_times
                    else 0
                ),
                "success_rate": f"{len(valid_results)}/{len(test_sentences)}",
                "errors": 0,
            }
        except Exception as e:
            error_msg = str(e).lower()
            if "502" in error_msg or "bad gateway" in error_msg:
                print(f"LLM {llm_name} g·∫∑p l·ªói 502, b·ªè qua ki·ªÉm tra")
            else:
                print(f"LLM {llm_name} ki·ªÉm tra th·∫•t b·∫°i: {str(e)}")
            error_type = "L·ªói m·∫°ng"
            if "timeout" in str(e).lower():
                error_type = "K·∫øt n·ªëi timeout"
            return {
                "name": llm_name,
                "type": "llm",
                "errors": 1,
                "error_type": error_type,
            }

    def _print_results(self):
        """In k·∫øt qu·∫£ ki·ªÉm tra"""
        print("\n" + "=" * 50)
        print("K·∫øt qu·∫£ ki·ªÉm tra hi·ªáu su·∫•t LLM")
        print("=" * 50)

        if not self.results:
            print("Kh√¥ng c√≥ k·∫øt qu·∫£ ki·ªÉm tra kh·∫£ d·ª•ng")
            return

        headers = ["T√™n m√¥ h√¨nh", "Th·ªùi gian ph·∫£n h·ªìi trung b√¨nh (s)", "Th·ªùi gian Token ƒë·∫ßu ti√™n (s)", "T·ª∑ l·ªá th√†nh c√¥ng", "Tr·∫°ng th√°i"]
        table_data = []

        # Thu th·∫≠p v√† ph√¢n lo·∫°i t·∫•t c·∫£ d·ªØ li·ªáu
        valid_results = []
        error_results = []

        for name, data in self.results.items():
            if data["errors"] == 0:
                # K·∫øt qu·∫£ b√¨nh th∆∞·ªùng
                avg_response = f"{data['avg_response']:.3f}"
                avg_first_token = (
                    f"{data['avg_first_token']:.3f}"
                    if data["avg_first_token"] > 0
                    else "-"
                )
                success_rate = data.get("success_rate", "N/A")
                status = "‚úÖ B√¨nh th∆∞·ªùng"

                # L∆∞u gi√° tr·ªã d√πng ƒë·ªÉ s·∫Øp x·∫øp
                first_token_value = (
                    data["avg_first_token"]
                    if data["avg_first_token"] > 0
                    else float("inf")
                )

                valid_results.append(
                    {
                        "name": name,
                        "avg_response": avg_response,
                        "avg_first_token": avg_first_token,
                        "success_rate": success_rate,
                        "status": status,
                        "sort_key": first_token_value,
                    }
                )
            else:
                # K·∫øt qu·∫£ l·ªói
                avg_response = "-"
                avg_first_token = "-"
                success_rate = "0/5"

                # L·∫•y lo·∫°i l·ªói c·ª• th·ªÉ
                error_type = data.get("error_type", "L·ªói m·∫°ng")
                status = f"‚ùå {error_type}"

                error_results.append(
                    [name, avg_response, avg_first_token, success_rate, status]
                )

        # S·∫Øp x·∫øp theo th·ªùi gian Token ƒë·∫ßu ti√™n tƒÉng d·∫ßn
        valid_results.sort(key=lambda x: x["sort_key"])

        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ h·ª£p l·ªá ƒë√£ s·∫Øp x·∫øp th√†nh d·ªØ li·ªáu b·∫£ng
        for result in valid_results:
            table_data.append(
                [
                    result["name"],
                    result["avg_response"],
                    result["avg_first_token"],
                    result["success_rate"],
                    result["status"],
                ]
            )

        # Th√™m k·∫øt qu·∫£ l·ªói v√†o cu·ªëi d·ªØ li·ªáu b·∫£ng
        table_data.extend(error_results)

        print(tabulate(table_data, headers=headers, tablefmt="grid"))
        print("\nH∆∞·ªõng d·∫´n ki·ªÉm tra:")
        print("- N·ªôi dung ki·ªÉm tra: K·ªãch b·∫£n ƒë·ªëi tho·∫°i agent bao g·ªìm system prompt ƒë·∫ßy ƒë·ªß")
        print("- Ki·ªÉm so√°t timeout: Th·ªùi gian ch·ªù t·ªëi ƒëa cho m·ªôt y√™u c·∫ßu l√† 10 gi√¢y")
        print("- X·ª≠ l√Ω l·ªói: T·ª± ƒë·ªông b·ªè qua c√°c m√¥ h√¨nh c√≥ l·ªói 502 v√† l·ªói m·∫°ng b·∫•t th∆∞·ªùng")
        print("- T·ª∑ l·ªá th√†nh c√¥ng: S·ªë l∆∞·ª£ng c√¢u ph·∫£n h·ªìi th√†nh c√¥ng / T·ªïng s·ªë c√¢u ki·ªÉm tra")
        print("\nKi·ªÉm tra ho√†n t·∫•t!")

    async def run(self):
        """Th·ª±c thi ki·ªÉm tra b·∫•t ƒë·ªìng b·ªô to√†n b·ªô"""
        print("B·∫Øt ƒë·∫ßu l·ªçc c√°c module LLM kh·∫£ d·ª•ng...")

        # T·∫°o t·∫•t c·∫£ c√°c task ki·ªÉm tra
        all_tasks = []

        # Task ki·ªÉm tra LLM
        if self.config.get("LLM") is not None:
            for llm_name, config in self.config.get("LLM", {}).items():
                # Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa c·∫•u h√¨nh
                if llm_name == "CozeLLM":
                    if any(x in config.get("bot_id", "") for x in ["‰Ω†ÁöÑ"]) or any(
                        x in config.get("user_id", "") for x in ["‰Ω†ÁöÑ"]
                    ):
                        print(f"LLM {llm_name} ch∆∞a c·∫•u h√¨nh bot_id/user_id, ƒë√£ b·ªè qua")
                        continue
                elif "api_key" in config and any(
                    x in config["api_key"] for x in ["‰Ω†ÁöÑ", "placeholder", "sk-xxx"]
                ):
                    print(f"LLM {llm_name} ch∆∞a c·∫•u h√¨nh api_key, ƒë√£ b·ªè qua")
                    continue

                # ƒê·ªëi v·ªõi Ollama, ki·ªÉm tra tr·∫°ng th√°i d·ªãch v·ª• tr∆∞·ªõc
                if llm_name == "Ollama":
                    base_url = config.get("base_url", "http://localhost:11434")
                    model_name = config.get("model_name")
                    if not model_name:
                        print("Ollama ch∆∞a c·∫•u h√¨nh model_name")
                        continue

                    if not await self._check_ollama_service(base_url, model_name):
                        continue

                print(f"Th√™m task ki·ªÉm tra LLM: {llm_name}")
                all_tasks.append(self._test_llm(llm_name, config))

        print(f"\nT√¨m th·∫•y {len(all_tasks)} module LLM kh·∫£ d·ª•ng")
        print("\nB·∫Øt ƒë·∫ßu ki·ªÉm tra ƒë·ªìng th·ªùi t·∫•t c·∫£ c√°c module...\n")

        # Th·ª±c thi t·∫•t c·∫£ c√°c task ki·ªÉm tra ƒë·ªìng th·ªùi, nh∆∞ng thi·∫øt l·∫≠p timeout ƒë·ªôc l·∫≠p cho m·ªói task
        async def test_with_timeout(task, timeout=30):
            """Th√™m b·∫£o v·ªá timeout cho m·ªói task ki·ªÉm tra"""
            try:
                return await asyncio.wait_for(task, timeout=timeout)
            except asyncio.TimeoutError:
                print(f"Task ki·ªÉm tra timeout ({timeout} gi√¢y), b·ªè qua")
                return {
                    "name": "Unknown",
                    "type": "llm",
                    "errors": 1,
                    "error_type": "K·∫øt n·ªëi timeout",
                }
            except Exception as e:
                print(f"Task ki·ªÉm tra b·∫•t th∆∞·ªùng: {str(e)}")
                return {
                    "name": "Unknown",
                    "type": "llm",
                    "errors": 1,
                    "error_type": "L·ªói m·∫°ng",
                }

        # B·ªçc b·∫£o v·ªá timeout cho m·ªói task
        protected_tasks = [test_with_timeout(task) for task in all_tasks]

        # Th·ª±c thi t·∫•t c·∫£ c√°c task ki·ªÉm tra ƒë·ªìng th·ªùi
        all_results = await asyncio.gather(*protected_tasks, return_exceptions=True)

        # X·ª≠ l√Ω k·∫øt qu·∫£
        for result in all_results:
            if isinstance(result, dict):
                if result.get("errors") == 0:
                    self.results[result["name"]] = result
                else:
                    # Ngay c·∫£ khi c√≥ l·ªói c≈©ng ghi l·∫°i, ƒë·ªÉ hi·ªÉn th·ªã tr·∫°ng th√°i th·∫•t b·∫°i
                    if result.get("name") != "Unknown":
                        self.results[result["name"]] = result
            elif isinstance(result, Exception):
                print(f"X·ª≠ l√Ω k·∫øt qu·∫£ ki·ªÉm tra b·∫•t th∆∞·ªùng: {str(result)}")

        # In k·∫øt qu·∫£
        print("\nƒêang t·∫°o b√°o c√°o ki·ªÉm tra...")
        self._print_results()


async def main():
    tester = LLMPerformanceTester()
    await tester.run()


if __name__ == "__main__":
    asyncio.run(main())
